import tensorflow as tf
from collections import OrderedDict


class OptimizerDict:

    def __init__(self, ts, dynamics):
        self._ts = ts
        self._dynamics = dynamics
        self._iteration = None
        self._initialization = None
        self._init_dyn = None  # for phi_0 (will be a dictionary (state-variable, phi_0 op)

    @property
    def ts(self):
        """
        Descent step, as returned by `tf.train.Optimizer.apply_gradients`.
        :return:
        """
        return self._ts

    @property
    def iteration(self):
        """
        Performs a descent step (as return by `tf.train.Optimizer.apply_gradients`) and computes the values of
        the variables after it.

        :return: A list of operation that, after performing one iteration, return the value of the state variables
                    being optimized (possibly including auxiliary variables)
        """
        if self._iteration is None:
            with tf.control_dependencies([self._ts]):
                self._iteration = self._state_read()  # performs an iteration and returns the
                # value of all variables in the state (ordered according to dyn)
        return self._iteration

    @property
    def initialization(self):
        """
        :return: a list of operations that return the values of the state variables for this
                    learning dynamics after the execution of the initialization operation. If
                    an initial dynamics is set, then it also executed.
        """
        if self._initialization is None:
            with tf.control_dependencies([tf.variables_initializer(self.state)]):
                if self._init_dyn is not None:  # create assign operation for initialization
                    self._initialization = [k.assign(v) for k, v in self._init_dyn.items()]
                    # return these new initialized values (and ignore variable initializers)
                else:
                    self._initialization = self._state_read()  # initialize state variables and
                    # return the initialized value

        return self._initialization

    @property
    def dynamics(self):
        """
        :return: The dynamics, a list of pairs (state_variable_k, state_variable_{k+1})
        """
        return [d for (v, d) in self._dynamics]

    @property
    def state(self):
        """
        :return: All the state variables (optimized variables and possibly auxiliary variables) being optimized
        """
        return [v for (v, d) in self._dynamics]  # overridden in Adam

    def _state_read(self):
        """
        :return: list of read value op for the state variables
        """
        return [v.read_value() for v in self.state]  # not sure about read_value vs value

    def state_feed_dict_generator(self, history):
        """
        Generator that yields a feed dictionary for the states being optimized, given an list of (previous) states,
        which typically are generated by the execution of `initialization` and `iteration`.

        :param history: a list of lists of values for the state variables, with the same order as `state`
        :return: a generator that yields the iteration and a feed dictionary with replacements for the state variables.
        """
        state = self.state
        for t, his in enumerate(history):
            yield t, {v: his[k] for k, v in enumerate(state)}

    def set_init_dynamics(self, init_dictionary):
        """
        With this function is possible to set an initializer for the dynamics. Multiple calls of this method on the
        same variable will override the dynamics.

        :param init_dictionary: a dictionary of (state_variable: tensor or variable, that represents the initial
                                dynamics Phi_0.
        """
        if self._init_dyn is None:
            self._init_dyn = OrderedDict([(v, tf.identity(v)) for v in self.state])  # do nothing
        for k, v in init_dictionary.items():
            assert k in self._init_dyn, 'Can set initial operation only for state variables in this object, got %s' % k
            self._init_dyn[k] = v

    @property
    def init_dynamics(self):
        """
        :return: The initialization dynamics if it has been set, or `None` otherwise.
        """
        return None if self._init_dyn is None else self._init_dyn.items()

    def __lt__(self, other):  # make OptimizerDict sortable
        assert isinstance(other, OptimizerDict)
        return hash(self) < hash(other)


# noinspection PyAbstractClass,PyClassHasNoInit
class Optimizer(tf.train.Optimizer):
    def minimize(self, loss, global_step=None, var_list=None, gate_gradients=tf.train.Optimizer.GATE_OP,
                 aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None):
        # compute_ddyn_dhypers=False, hyperparameters=None):  (looks like these parameters are no longer needed)
        """
        Returns a training step operation and the training dynamics in the form of
        list of var_and_dynamics where var are both variables in `var_list` and also additional state (auxiliary)
        variables needed

        See tf.train.Optimizer.minimize.  Adds the computation of B_t if forward_hg is `True`
        """
        ts, dyn = super().minimize(loss, global_step, var_list, gate_gradients, aggregation_method,
                                   colocate_gradients_with_ops, name, grad_loss)
        # ddyn_dhypers = self._compute_ddyn_dhypers(loss, colocate_gradients_with_ops, aggregation_method,
        #                                           gate_gradients, hyperparameters) if compute_ddyn_dhypers else None
        return OptimizerDict(ts=ts, dynamics=dyn)  # ddyn_dhypers=ddyn_dhypers)

    # def _compute_ddyn_dhypers(self, loss, colocate_gradients_with_ops=None, aggregation_method=None,
    #                           gate_gradients=None, hyperparameters=None):
    #     hypers = hyperparameters or tf.get_collection(GraphKeys.HYPERPARAMETERS)
    #     assert all([hy.get_shape().ndims == 1 for hy in hypers]), 'only scalar hyperparameters for now'
    #     grad_and_hypers = self.compute_gradients(loss, hypers, gate_gradients, aggregation_method,
    #                                              colocate_gradients_with_ops)
    #     # TODO filter for algorithmic hyperparameters (grad would be None here!)
    #     return [
    #         (tf.gradients(g, hypers, name='d_dyn_d_hypers'), v) for (g, v) in grad_and_hypers
    #     ]


# noinspection PyClassHasNoInit,PyAbstractClass
class GradientDescentOptimizer(Optimizer, tf.train.GradientDescentOptimizer):
    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
        ts = super().apply_gradients(grads_and_vars, global_step, name)
        dynamics = []
        for g, w in grads_and_vars:
            wk = w - self._learning_rate_tensor * g
            dynamics.append((w, wk))
        return ts, dynamics


class MomentumOptimizer(Optimizer, tf.train.MomentumOptimizer):
    def __init__(self, learning_rate, momentum, use_locking=False, name="Momentum",
                 use_nesterov=False):
        assert use_nesterov is False, 'Nesterov momentum not implemented yet...'
        super().__init__(learning_rate, momentum, use_locking, name, use_nesterov)

    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
        #         filter_hypers

        ts = super().apply_gradients(grads_and_vars, global_step, name)

        # builds up the dynamics here
        mn = self.get_slot_names()[0]
        dynamics = []
        for g, w in grads_and_vars:
            m = self.get_slot(w, mn)
            mk = self._momentum_tensor * m + g
            wk = w - self._learning_rate_tensor * mk
            dynamics.extend([(w, wk), (m, mk)])

        return ts, dynamics


# noinspection PyClassHasNoInit
class AdamOptimizer(Optimizer, tf.train.AdamOptimizer):
    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
        ts = super().apply_gradients(grads_and_vars, global_step, name)

        mn, vn = self.get_slot_names()
        dynamics = []
        for g, w in grads_and_vars:
            m = self.get_slot(w, mn)
            v = self.get_slot(w, vn)
            mk = self._beta1_t * m + (1. - self._beta1_t) * g
            vk = self._beta2_t * v + (1. - self._beta2_t) * g * g
            wk = w - self._lr_t * mk / (tf.sqrt(vk + self._epsilon_t)) # if epsilon outside sqrt hypergradient has nan
            dynamics.extend([(w, wk), (m, mk), (v, vk)])
        b1_pow, b2_pow = self._get_beta_accumulators()
        b1_powk = b1_pow * self._beta1_t
        b2_powk = b2_pow * self._beta2_t
        dynamics.extend([(b1_pow, b1_powk), (b2_pow, b2_powk)])

        return ts, dynamics
