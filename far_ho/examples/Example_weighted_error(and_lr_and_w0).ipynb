{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import far_ho as far\n",
    "import tensorflow as tf\n",
    "import far_ho.examples as far_ex\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "ss = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 28**2), name='x')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\lfranceschi\\PycharmProjects\\FAR-HO\\far_ho\\examples\\MNIST_DATA\\train-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\lfranceschi\\PycharmProjects\\FAR-HO\\far_ho\\examples\\MNIST_DATA\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:\\Users\\lfranceschi\\PycharmProjects\\FAR-HO\\far_ho\\examples\\MNIST_DATA\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\lfranceschi\\PycharmProjects\\FAR-HO\\far_ho\\examples\\MNIST_DATA\\t10k-labels-idx1-ubyte.gz\n",
      "datasets.redivide_data:, computed partitions numbers - [0, 7000, 14000, 70000] len all 70000 DONE\n"
     ]
    }
   ],
   "source": [
    "# load a small portion of mnist data\n",
    "datasets = far_ex.mnist(folder=os.path.join(os.getcwd(), 'MNIST_DATA'), partitions=(.1, .1,))\n",
    "datasets = far_ex.Datasets.from_list(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground model weights (parameters)\n",
      "<tf.Variable 'model/fully_connected/weights:0' shape=(784, 300) dtype=float32_ref>\n",
      "<tf.Variable 'model/fully_connected/biases:0' shape=(300,) dtype=float32_ref>\n",
      "<tf.Variable 'model/fully_connected_1/weights:0' shape=(300, 10) dtype=float32_ref>\n",
      "<tf.Variable 'model/fully_connected_1/biases:0' shape=(10,) dtype=float32_ref>\n",
      "Initial model weights (hyperparameters)\n",
      "<tf.Variable 'inital_weight_model/fully_connected/weights:0' shape=(784, 300) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected/biases:0' shape=(300,) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected_1/weights:0' shape=(300, 10) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected_1/biases:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# build up a feddforward NN calssifier\n",
    "import tensorflow.contrib.layers as tcl\n",
    "with tf.variable_scope('model'):\n",
    "    h1 = tcl.fully_connected(x, 300)\n",
    "    out = tcl.fully_connected(h1, datasets.train.dim_target)\n",
    "    print('Ground model weights (parameters)')\n",
    "    [print(e) for e in tf.model_variables()];\n",
    "with tf.variable_scope('inital_weight_model'):\n",
    "    h1_hyp = tcl.fully_connected(x, 300,\n",
    "                                 variables_collections=[far.utils.GraphKeys.HYPERPARAMETERS, \n",
    "                                                       far.utils.GraphKeys.GLOBAL_VARIABLES], \n",
    "                                trainable=False)\n",
    "    out_hyp = tcl.fully_connected(h1_hyp, datasets.train.dim_target,\n",
    "                                 variables_collections=[far.utils.GraphKeys.HYPERPARAMETERS, \n",
    "                                                       far.utils.GraphKeys.GLOBAL_VARIABLES],\n",
    "                                 trainable=False)\n",
    "    print('Initial model weights (hyperparameters)')\n",
    "    [print(e) for e in far.utils.hyperparameters()];\n",
    "#     far.utils.remove_from_collection(far.GraphKeys.MODEL_VARIABLES, *far.utils.hyperparameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get an hyperparameter for weighting the examples for the inner objective loss (training error)\n",
    "weights = far.get_hyperparameter('ex_weights', tf.zeros(datasets.train.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build loss and accuracy \n",
    "# inner objective (training error), weighted mean of cross entropy errors (with sigmoid to be sure is > 0)\n",
    "with tf.name_scope('errors'):\n",
    "    tr_loss = tf.reduce_mean(tf.sigmoid(weights)*tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))\n",
    "    # outer objective (validation error) (not weighted)\n",
    "    val_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(out, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizers\n",
    "# get an hyperparameter for the learning rate\n",
    "lr = far.get_hyperparameter('lr', 0.01)\n",
    "io_optim = far.GradientDescentOptimizer(lr)  # for training error minimization an optimizer from far_ho is needed\n",
    "oo_optim = tf.train.AdamOptimizer()  # for outer objective optimizer all optimizers from tf are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters to optimize\n",
      "<tf.Variable 'inital_weight_model/fully_connected/weights:0' shape=(784, 300) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected/biases:0' shape=(300,) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected_1/weights:0' shape=(300, 10) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected_1/biases:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'ex_weights:0' shape=(7000,) dtype=float32_ref>\n",
      "<tf.Variable 'lr:0' shape=() dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print('hyperparameters to optimize')\n",
    "[print(h) for h in far.utils.hyperparameters()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build hyperparameter optimizer\n",
    "farho = far.HyperOptimizer()\n",
    "run = farho.minimize(val_loss, oo_optim, tr_loss, io_optim, \n",
    "                     init_dynamics_dict={v: h for v, h in zip(tf.model_variables(), far.utils.hyperparameters()[:4])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy 0.12\n",
      "validation accuracy 0.117143\n",
      "--------------------------------------------------\n",
      "training accuracy 0.482857\n",
      "validation accuracy 0.454857\n",
      "learning rate 0.011\n",
      "norm of examples weight 0.0796879\n",
      "--------------------------------------------------\n",
      "training accuracy 0.611857\n",
      "validation accuracy 0.589286\n",
      "learning rate 0.0120009\n",
      "norm of examples weight 0.152052\n",
      "--------------------------------------------------\n",
      "training accuracy 0.662714\n",
      "validation accuracy 0.645\n",
      "learning rate 0.0129937\n",
      "norm of examples weight 0.2271\n",
      "--------------------------------------------------\n",
      "training accuracy 0.693571\n",
      "validation accuracy 0.681714\n",
      "learning rate 0.0139686\n",
      "norm of examples weight 0.302777\n",
      "--------------------------------------------------\n",
      "training accuracy 0.72\n",
      "validation accuracy 0.711857\n",
      "learning rate 0.0149181\n",
      "norm of examples weight 0.377813\n",
      "--------------------------------------------------\n",
      "training accuracy 0.766571\n",
      "validation accuracy 0.759714\n",
      "learning rate 0.0158564\n",
      "norm of examples weight 0.446893\n",
      "--------------------------------------------------\n",
      "training accuracy 0.822143\n",
      "validation accuracy 0.813571\n",
      "learning rate 0.0167774\n",
      "norm of examples weight 0.514196\n",
      "--------------------------------------------------\n",
      "training accuracy 0.840143\n",
      "validation accuracy 0.831286\n",
      "learning rate 0.0176722\n",
      "norm of examples weight 0.58051\n",
      "--------------------------------------------------\n",
      "training accuracy 0.851714\n",
      "validation accuracy 0.843143\n",
      "learning rate 0.0185335\n",
      "norm of examples weight 0.645746\n",
      "--------------------------------------------------\n",
      "training accuracy 0.859714\n",
      "validation accuracy 0.850857\n",
      "learning rate 0.0193564\n",
      "norm of examples weight 0.709646\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# run hyperparameter optimization \n",
    "T = 100 # performs 100 iteraitons of gradient descent on the training error \n",
    "# get data suppliers (could also be stochastic for SGD)\n",
    "tr_supplier = datasets.train.create_supplier(x, y)\n",
    "val_supplier = datasets.validation.create_supplier(x, y)\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print('training accuracy', accuracy.eval(tr_supplier()))\n",
    "print('validation accuracy', accuracy.eval(val_supplier()))\n",
    "print('-'*50)\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    run(T, inner_objective_feed_dicts=tr_supplier, outer_objective_feed_dicts=val_supplier)\n",
    "    print('training accuracy', accuracy.eval(tr_supplier()))\n",
    "    print('validation accuracy', accuracy.eval(val_supplier()))\n",
    "    print('learning rate', lr.eval())\n",
    "    print('norm of examples weight', tf.norm(weights).eval())\n",
    "    print('-'*50)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
