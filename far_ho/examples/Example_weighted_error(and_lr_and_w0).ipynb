{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder is /media/luca/DATA/DATASETS\n",
      "Experiment save directory is  /media/luca/DATA/EXPERIMENTS\n"
     ]
    }
   ],
   "source": [
    "import far_ho as far\n",
    "import tensorflow as tf\n",
    "import far_ho.examples as far_ex\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "ss = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 28**2), name='x')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /media/luca/DATA/Progs/FAR-HO/far_ho/examples/MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting /media/luca/DATA/Progs/FAR-HO/far_ho/examples/MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting /media/luca/DATA/Progs/FAR-HO/far_ho/examples/MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting /media/luca/DATA/Progs/FAR-HO/far_ho/examples/MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "datasets.redivide_data:, computed partitions numbers - [0, 7000, 14000, 70000] len all 70000 DONE\n"
     ]
    }
   ],
   "source": [
    "# load a small portion of mnist data\n",
    "datasets = far_ex.mnist(folder=os.path.join(os.getcwd(), 'MNIST_DATA'), partitions=(.1, .1,))\n",
    "datasets = far_ex.Datasets.from_list(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground model weights (parameters)\n",
      "<tf.Variable 'model/fully_connected/weights:0' shape=(784, 300) dtype=float32_ref>\n",
      "<tf.Variable 'model/fully_connected/biases:0' shape=(300,) dtype=float32_ref>\n",
      "<tf.Variable 'model/fully_connected_1/weights:0' shape=(300, 10) dtype=float32_ref>\n",
      "<tf.Variable 'model/fully_connected_1/biases:0' shape=(10,) dtype=float32_ref>\n",
      "Initial model weights (hyperparameters)\n",
      "<tf.Variable 'inital_weight_model/fully_connected/weights:0' shape=(784, 300) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected/biases:0' shape=(300,) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected_1/weights:0' shape=(300, 10) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected_1/biases:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# build up a feddforward NN calssifier\n",
    "import tensorflow.contrib.layers as tcl\n",
    "with tf.variable_scope('model'):\n",
    "    h1 = tcl.fully_connected(x, 300)\n",
    "    out = tcl.fully_connected(h1, datasets.train.dim_target)\n",
    "    print('Ground model weights (parameters)')\n",
    "    [print(e) for e in tf.model_variables()];\n",
    "with tf.variable_scope('inital_weight_model'):\n",
    "    h1_hyp = tcl.fully_connected(x, 300,\n",
    "                                 variables_collections=far.HYPERPARAMETERS_COLLECTIONS, \n",
    "                                trainable=False)\n",
    "    out_hyp = tcl.fully_connected(h1_hyp, datasets.train.dim_target,\n",
    "                                 variables_collections=far.HYPERPARAMETERS_COLLECTIONS,\n",
    "                                 trainable=False)\n",
    "    print('Initial model weights (hyperparameters)')\n",
    "    [print(e) for e in far.utils.hyperparameters()];\n",
    "#     far.utils.remove_from_collection(far.GraphKeys.MODEL_VARIABLES, *far.utils.hyperparameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get an hyperparameter for weighting the examples for the inner objective loss (training error)\n",
    "weights = far.get_hyperparameter('ex_weights', tf.zeros(datasets.train.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build loss and accuracy \n",
    "# inner objective (training error), weighted mean of cross entropy errors (with sigmoid to be sure is > 0)\n",
    "with tf.name_scope('errors'):\n",
    "    tr_loss = tf.reduce_mean(tf.sigmoid(weights)*tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))\n",
    "    # outer objective (validation error) (not weighted)\n",
    "    val_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(out, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizers\n",
    "# get an hyperparameter for the learning rate\n",
    "lr = far.get_hyperparameter('lr', 0.01)\n",
    "io_optim = far.GradientDescentOptimizer(lr)  # for training error minimization an optimizer from far_ho is needed\n",
    "oo_optim = tf.train.AdamOptimizer()  # for outer objective optimizer all optimizers from tf are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters to optimize\n",
      "<tf.Variable 'inital_weight_model/fully_connected/weights:0' shape=(784, 300) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected/biases:0' shape=(300,) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected_1/weights:0' shape=(300, 10) dtype=float32_ref>\n",
      "<tf.Variable 'inital_weight_model/fully_connected_1/biases:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'ex_weights:0' shape=(7000,) dtype=float32_ref>\n",
      "<tf.Variable 'lr:0' shape=() dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print('hyperparameters to optimize')\n",
    "[print(h) for h in far.hyperparameters()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build hyperparameter optimizer\n",
    "farho = far.HyperOptimizer()\n",
    "run = farho.minimize(val_loss, oo_optim, tr_loss, io_optim, \n",
    "                     init_dynamics_dict={v: h for v, h in zip(tf.model_variables(), far.utils.hyperparameters()[:4])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables (or tensors) that will store the values of the hypergradients\n",
      "Tensor(\"errors/Mean_1_1/gradients_3/errors/Mean_1_1/Vectorization_2/Reshape_grad/Reshape:0\", shape=(784, 300), dtype=float32)\n",
      "Tensor(\"errors/Mean_1_1/gradients_3/errors/Mean_1_1/Vectorization_2/Reshape_1_grad/Reshape:0\", shape=(300,), dtype=float32)\n",
      "Tensor(\"errors/Mean_1_1/gradients_3/errors/Mean_1_1/Vectorization_2/Reshape_2_grad/Reshape:0\", shape=(300, 10), dtype=float32)\n",
      "Tensor(\"errors/Mean_1_1/gradients_3/errors/Mean_1_1/Vectorization_2/Reshape_3_grad/Reshape:0\", shape=(10,), dtype=float32)\n",
      "<tf.Variable 'errors/Mean_1/ex_weights/hypergradient:0' shape=(7000,) dtype=float32_ref>\n",
      "<tf.Variable 'errors/Mean_1/lr/hypergradient:0' shape=() dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print('Variables (or tensors) that will store the values of the hypergradients')\n",
    "print(*far.hypergradients(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy 0.101571\n",
      "validation accuracy 0.0938571\n",
      "--------------------------------------------------\n",
      "training accuracy 0.631143\n",
      "validation accuracy 0.610429\n",
      "learning rate 0.011\n",
      "norm of examples weight 0.0819767\n",
      "--------------------------------------------------\n",
      "training accuracy 0.754286\n",
      "validation accuracy 0.731143\n",
      "learning rate 0.0120004\n",
      "norm of examples weight 0.157757\n",
      "--------------------------------------------------\n",
      "training accuracy 0.806572\n",
      "validation accuracy 0.778143\n",
      "learning rate 0.0129907\n",
      "norm of examples weight 0.235425\n",
      "--------------------------------------------------\n",
      "training accuracy 0.828857\n",
      "validation accuracy 0.803143\n",
      "learning rate 0.0139599\n",
      "norm of examples weight 0.313249\n",
      "--------------------------------------------------\n",
      "training accuracy 0.843572\n",
      "validation accuracy 0.822\n",
      "learning rate 0.0148998\n",
      "norm of examples weight 0.390272\n",
      "--------------------------------------------------\n",
      "training accuracy 0.856715\n",
      "validation accuracy 0.838143\n",
      "learning rate 0.0158043\n",
      "norm of examples weight 0.465889\n",
      "--------------------------------------------------\n",
      "training accuracy 0.866143\n",
      "validation accuracy 0.849857\n",
      "learning rate 0.0166697\n",
      "norm of examples weight 0.539688\n",
      "--------------------------------------------------\n",
      "training accuracy 0.874714\n",
      "validation accuracy 0.859143\n",
      "learning rate 0.017494\n",
      "norm of examples weight 0.611407\n",
      "--------------------------------------------------\n",
      "training accuracy 0.881572\n",
      "validation accuracy 0.866714\n",
      "learning rate 0.0182761\n",
      "norm of examples weight 0.680871\n",
      "--------------------------------------------------\n",
      "training accuracy 0.885429\n",
      "validation accuracy 0.874572\n",
      "learning rate 0.0190163\n",
      "norm of examples weight 0.747985\n",
      "--------------------------------------------------\n",
      "training accuracy 0.887715\n",
      "validation accuracy 0.879857\n",
      "learning rate 0.0197151\n",
      "norm of examples weight 0.812704\n",
      "--------------------------------------------------\n",
      "training accuracy 0.890572\n",
      "validation accuracy 0.884714\n",
      "learning rate 0.0203736\n",
      "norm of examples weight 0.875019\n",
      "--------------------------------------------------\n",
      "training accuracy 0.894429\n",
      "validation accuracy 0.890143\n",
      "learning rate 0.0209933\n",
      "norm of examples weight 0.934947\n",
      "--------------------------------------------------\n",
      "training accuracy 0.897\n",
      "validation accuracy 0.895715\n",
      "learning rate 0.0215758\n",
      "norm of examples weight 0.992525\n",
      "--------------------------------------------------\n",
      "training accuracy 0.901\n",
      "validation accuracy 0.900572\n",
      "learning rate 0.0221227\n",
      "norm of examples weight 1.0478\n",
      "--------------------------------------------------\n",
      "training accuracy 0.903143\n",
      "validation accuracy 0.904\n",
      "learning rate 0.0226358\n",
      "norm of examples weight 1.10084\n",
      "--------------------------------------------------\n",
      "training accuracy 0.905429\n",
      "validation accuracy 0.905143\n",
      "learning rate 0.023117\n",
      "norm of examples weight 1.15171\n",
      "--------------------------------------------------\n",
      "training accuracy 0.907572\n",
      "validation accuracy 0.909572\n",
      "learning rate 0.0235681\n",
      "norm of examples weight 1.20049\n",
      "--------------------------------------------------\n",
      "training accuracy 0.909\n",
      "validation accuracy 0.912\n",
      "learning rate 0.0239907\n",
      "norm of examples weight 1.24727\n",
      "--------------------------------------------------\n",
      "training accuracy 0.911\n",
      "validation accuracy 0.915572\n",
      "learning rate 0.0243866\n",
      "norm of examples weight 1.29212\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# run hyperparameter optimization \n",
    "T = 200 # performs 200 iteraitons of gradient descent on the training error (rise this values for better performances)\n",
    "# get data suppliers (could also be stochastic for SGD)\n",
    "tr_supplier = datasets.train.create_supplier(x, y)\n",
    "val_supplier = datasets.validation.create_supplier(x, y)\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print('training accuracy', accuracy.eval(tr_supplier()))\n",
    "print('validation accuracy', accuracy.eval(val_supplier()))\n",
    "print('-'*50)\n",
    "\n",
    "\n",
    "for _ in range(20):\n",
    "    run(T, inner_objective_feed_dicts=tr_supplier, outer_objective_feed_dicts=val_supplier)\n",
    "    print('training accuracy', accuracy.eval(tr_supplier()))\n",
    "    print('validation accuracy', accuracy.eval(val_supplier()))\n",
    "    print('learning rate', lr.eval())\n",
    "    print('norm of examples weight', tf.norm(weights).eval())\n",
    "    print('-'*50)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
